{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AML - VOC 2012 - test set evaluation"
      ],
      "metadata": {
        "id": "FUyoVEVXrmwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and utility"
      ],
      "metadata": {
        "id": "PeSw2T35rspZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "dTODzbFRr0cF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (128, 128)"
      ],
      "metadata": {
        "id": "b9xZdGMusFEI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = '/content/drive/MyDrive/'\n",
        "BASE_DIR = ROOT_DIR + 'project/test/'\n",
        "PARTITION_FILENAMES = [f\"voc_test_partition_{i}.npz\" for i in range(3)]\n",
        "PARTITION_PATHS = [os.path.join(BASE_DIR, filename) for filename in PARTITION_FILENAMES]\n",
        "\n",
        "PARTITION_PATHS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOz2ZLh3sKYM",
        "outputId": "fd390d10-1fb5-4d88-9bc3-8e97738b9235"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/project/test/voc_test_partition_0.npz',\n",
              " '/content/drive/MyDrive/project/test/voc_test_partition_1.npz',\n",
              " '/content/drive/MyDrive/project/test/voc_test_partition_2.npz']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_images(images: np.ndarray, resize=IMG_SIZE):\n",
        "    images /= 255.0\n",
        "    resized_images = []\n",
        "\n",
        "    for i in range(images.shape[0]):\n",
        "        resized_image = tf.image.resize(images[i], resize).numpy()\n",
        "        resized_images.append(resized_image)\n",
        "\n",
        "    resized_images = np.array(resized_images)\n",
        "    del images\n",
        "\n",
        "    return resized_images"
      ],
      "metadata": {
        "id": "BAEOnlZGtUPd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYMn6FlSsNAZ",
        "outputId": "90cb515d-4e9b-4078-d447-48fb44a41a9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = [\n",
        "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
        "    \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "]\n",
        "PRED_FOLDER = \"./results/VOC2012/Main\"\n",
        "FILE_FORMAT = \"comp1_cls_test_{}.txt\""
      ],
      "metadata": {
        "id": "rN4V6_v6OwVd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def serialize_predictions(model_filepath, partition_paths, classes=CLASSES, pred_folder=PRED_FOLDER, file_format=FILE_FORMAT):\n",
        "  preds = []\n",
        "  filenames = []\n",
        "\n",
        "  print(f\"Loading model from '{model_filepath}'\")\n",
        "  model = tf.keras.models.load_model(model_filepath)\n",
        "\n",
        "  for i, partition in enumerate(partition_paths):\n",
        "    print(f\"Partition {i + 1}/{len(partition_paths)}:\")\n",
        "    print(f\"\\tLoading partition...\")\n",
        "    partition_data = np.load(partition, allow_pickle=True)\n",
        "    partition_images = partition_data['images']\n",
        "    partition_filenames = partition_data['filenames']\n",
        "\n",
        "    print(f\"\\tPreprocessing partition...\")\n",
        "    partition_images = preprocess_images(partition_images)\n",
        "\n",
        "    print(f\"\\tComputing predictions...\")\n",
        "    partition_preds = model.predict(partition_images)\n",
        "    del partition_images\n",
        "\n",
        "    preds.append(partition_preds)\n",
        "    filenames.append(partition_filenames)\n",
        "\n",
        "  print(f\"Concatenating partitions predictions...\")\n",
        "  preds = np.concatenate(preds)\n",
        "  filenames = np.concatenate(filenames)\n",
        "\n",
        "  print(f\"Sorting predictions by filename...\")\n",
        "  indices = np.argsort(filenames)\n",
        "  preds = preds[indices]\n",
        "  filenames = filenames[indices]\n",
        "\n",
        "  print(\"Serializing predictions...\")\n",
        "  os.makedirs(pred_folder, exist_ok=True)\n",
        "  preds_t = preds.T\n",
        "  for i, c in enumerate(classes):\n",
        "    file_path = os.path.join(pred_folder, file_format.format(c))\n",
        "    print(f\"\\tSerializing predictions for class '{c}' to '{file_path}'\")\n",
        "    f = open(file_path, 'w')\n",
        "    for filename, pred in zip(filenames, preds_t[i]):\n",
        "      f.write(f\"{filename} {pred}\\n\")\n",
        "    f.close()\n",
        "\n",
        "  del preds, filenames"
      ],
      "metadata": {
        "id": "YgG2Sbmm-PCX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = \"baseline_augmented_model\""
      ],
      "metadata": {
        "id": "zXnrKuTYPUzF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "serialize_predictions(f\"{model_filename}.keras\", PARTITION_PATHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af90d50-1bb5-4952-a38c-265827c55b8f",
        "id": "2GsM7JR_EkXP"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from 'baseline_augmented_model.keras'\n",
            "Partition 1/3:\n",
            "\tLoading partition...\n",
            "\tPreprocessing partition...\n",
            "\tComputing predictions...\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step\n",
            "Partition 2/3:\n",
            "\tLoading partition...\n",
            "\tPreprocessing partition...\n",
            "\tComputing predictions...\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n",
            "Partition 3/3:\n",
            "\tLoading partition...\n",
            "\tPreprocessing partition...\n",
            "\tComputing predictions...\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step\n",
            "Concatenating partitions predictions...\n",
            "Sorting predictions by filename...\n",
            "Serializing predictions...\n",
            "\tSerializing predictions for class 'aeroplane' to './results/VOC2012/Main/comp1_cls_test_aeroplane.txt'\n",
            "\tSerializing predictions for class 'bicycle' to './results/VOC2012/Main/comp1_cls_test_bicycle.txt'\n",
            "\tSerializing predictions for class 'bird' to './results/VOC2012/Main/comp1_cls_test_bird.txt'\n",
            "\tSerializing predictions for class 'boat' to './results/VOC2012/Main/comp1_cls_test_boat.txt'\n",
            "\tSerializing predictions for class 'bottle' to './results/VOC2012/Main/comp1_cls_test_bottle.txt'\n",
            "\tSerializing predictions for class 'bus' to './results/VOC2012/Main/comp1_cls_test_bus.txt'\n",
            "\tSerializing predictions for class 'car' to './results/VOC2012/Main/comp1_cls_test_car.txt'\n",
            "\tSerializing predictions for class 'cat' to './results/VOC2012/Main/comp1_cls_test_cat.txt'\n",
            "\tSerializing predictions for class 'chair' to './results/VOC2012/Main/comp1_cls_test_chair.txt'\n",
            "\tSerializing predictions for class 'cow' to './results/VOC2012/Main/comp1_cls_test_cow.txt'\n",
            "\tSerializing predictions for class 'diningtable' to './results/VOC2012/Main/comp1_cls_test_diningtable.txt'\n",
            "\tSerializing predictions for class 'dog' to './results/VOC2012/Main/comp1_cls_test_dog.txt'\n",
            "\tSerializing predictions for class 'horse' to './results/VOC2012/Main/comp1_cls_test_horse.txt'\n",
            "\tSerializing predictions for class 'motorbike' to './results/VOC2012/Main/comp1_cls_test_motorbike.txt'\n",
            "\tSerializing predictions for class 'person' to './results/VOC2012/Main/comp1_cls_test_person.txt'\n",
            "\tSerializing predictions for class 'pottedplant' to './results/VOC2012/Main/comp1_cls_test_pottedplant.txt'\n",
            "\tSerializing predictions for class 'sheep' to './results/VOC2012/Main/comp1_cls_test_sheep.txt'\n",
            "\tSerializing predictions for class 'sofa' to './results/VOC2012/Main/comp1_cls_test_sofa.txt'\n",
            "\tSerializing predictions for class 'train' to './results/VOC2012/Main/comp1_cls_test_train.txt'\n",
            "\tSerializing predictions for class 'tvmonitor' to './results/VOC2012/Main/comp1_cls_test_tvmonitor.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_filename = f'results.tgz'\n",
        "with tarfile.open(output_filename, \"w:gz\") as tar:\n",
        "    tar.add(PRED_FOLDER, arcname=PRED_FOLDER)\n",
        "\n",
        "destination = BASE_DIR + f\"{model_filename}_{output_filename}\"\n",
        "shutil.move(output_filename, destination)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bAfowWfMR4jt",
        "outputId": "6d6b5264-dc54-469b-a97a-089c09976256"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/project/test/baseline_augmented_model_results.tgz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}